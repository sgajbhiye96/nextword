{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zagymX2RKxhI",
        "outputId": "30d941ce-1a23-4b74-8908-85e1c35aea44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large dummy dataset created!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "subjects = [\n",
        "    \"deep learning\", \"machine learning\", \"artificial intelligence\",\n",
        "    \"data science\", \"neural networks\", \"computer vision\",\n",
        "    \"natural language processing\", \"big data\", \"cloud computing\"\n",
        "]\n",
        "\n",
        "verbs = [\n",
        "    \"is used for\", \"helps in\", \"improves\", \"supports\",\n",
        "    \"enables\", \"optimizes\", \"transforms\"\n",
        "]\n",
        "\n",
        "objects = [\n",
        "    \"image recognition\", \"text analysis\", \"speech recognition\",\n",
        "    \"prediction systems\", \"automation\", \"decision making\",\n",
        "    \"pattern recognition\"\n",
        "]\n",
        "\n",
        "extras = [\n",
        "    \"in modern applications\",\n",
        "    \"for real world problems\",\n",
        "    \"using large datasets\",\n",
        "    \"with high accuracy\",\n",
        "    \"in production systems\",\n",
        "    \"at scale\"\n",
        "]\n",
        "\n",
        "def generate_sentence():\n",
        "    return f\"{random.choice(subjects)} {random.choice(verbs)} {random.choice(objects)} {random.choice(extras)}\"\n",
        "\n",
        "# Generate BIG dataset\n",
        "with open(\"large_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for _ in range(100_000):   # ğŸ”¥ BIG DATA\n",
        "        f.write(generate_sentence() + \"\\n\")\n",
        "\n",
        "print(\"Large dummy dataset created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "E9khLP13K9mD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(\n",
        "    num_words=20000,\n",
        "    oov_token=\"<OOV>\"\n",
        ")\n",
        "\n",
        "with open(\"large_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        tokenizer.fit_on_texts([line.lower()])\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(\"Vocab size:\", total_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQKmsf4MLB2n",
        "outputId": "6ced0962-3aaa-4700-f6d3-55abb075782e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "with open(\"large_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        seq = tokenizer.texts_to_sequences([line])[0]\n",
        "        max_len = max(max_len, len(seq))\n",
        "\n",
        "print(\"Max length:\", max_len)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x28FB_XTLD_j",
        "outputId": "320503b1-9612-45ee-ca38-6bc68e64090f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(file_path, tokenizer, max_len, batch_size=128):\n",
        "    sequences = []\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            token_list = tokenizer.texts_to_sequences([line.lower()])[0]\n",
        "\n",
        "            for i in range(1, len(token_list)):\n",
        "                sequences.append(token_list[:i+1])\n",
        "\n",
        "                if len(sequences) == batch_size:\n",
        "                    padded = pad_sequences(sequences, maxlen=max_len, padding=\"pre\")\n",
        "                    X = padded[:, :-1]\n",
        "                    y = padded[:, -1]\n",
        "                    y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "                    yield X, y\n",
        "                    sequences = []\n"
      ],
      "metadata": {
        "id": "tb0T5YanLGfI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(\"large_corpus.txt\", tokenizer, max_len),\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=((None, max_len-1), (None, total_words))\n",
        ").prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLChwxJ3LKhV",
        "outputId": "c6cfcbe2-dc99-430e-c69f-bb82a45dc30a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-2231041970.py:1: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use output_signature instead\n",
            "WARNING:tensorflow:From /tmp/ipython-input-2231041970.py:1: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use output_signature instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 128),\n",
        "    tf.keras.layers.SimpleRNN(128),\n",
        "    tf.keras.layers.Dense(total_words, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "rnn_model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "rnn_model.fit(dataset, epochs=5, steps_per_epoch=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIW2OWMaLMs9",
        "outputId": "be670f97-9b90-4fc0-f597-10e219fb3401"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - accuracy: 0.5579 - loss: 1.3597\n",
            "Epoch 2/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - accuracy: 0.6526 - loss: 0.7796\n",
            "Epoch 3/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.6558 - loss: 0.7759\n",
            "Epoch 4/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.6578 - loss: 0.7756\n",
            "Epoch 5/5\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.6566 - loss: 0.7770\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7919d373cbc0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 128),\n",
        "    tf.keras.layers.LSTM(256),\n",
        "    tf.keras.layers.Dense(total_words, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "lstm_model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "lstm_model.fit(dataset, epochs=8, steps_per_epoch=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NEVUweTLQA6",
        "outputId": "e75377a1-728f-485e-fd73-9d756eecfa39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 76ms/step - accuracy: 0.5087 - loss: 1.5396\n",
            "Epoch 2/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 74ms/step - accuracy: 0.6521 - loss: 0.7816\n",
            "Epoch 3/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 74ms/step - accuracy: 0.6550 - loss: 0.7777\n",
            "Epoch 4/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 83ms/step - accuracy: 0.6565 - loss: 0.7772\n",
            "Epoch 5/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 82ms/step - accuracy: 0.6559 - loss: 0.7785\n",
            "Epoch 6/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 64ms/step - accuracy: 0.6559 - loss: 0.7765\n",
            "Epoch 7/8\n",
            "\u001b[1m   2/1000\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:20\u001b[0m 81ms/step - accuracy: 0.6621 - loss: 0.7885"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 90ms/step - accuracy: 0.6537 - loss: 0.7766\n",
            "Epoch 8/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 75ms/step - accuracy: 0.6544 - loss: 0.7752\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7919b943f020>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 128),\n",
        "    tf.keras.layers.GRU(256),\n",
        "    tf.keras.layers.Dense(total_words, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "gru_model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "gru_model.fit(dataset, epochs=8, steps_per_epoch=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoEQ8C4_LUXx",
        "outputId": "77ff6abb-9de1-4011-fff2-a4ea3420d6f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 83ms/step - accuracy: 0.5580 - loss: 1.3439\n",
            "Epoch 2/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 58ms/step - accuracy: 0.6522 - loss: 0.7810\n",
            "Epoch 3/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 57ms/step - accuracy: 0.6550 - loss: 0.7776\n",
            "Epoch 4/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 57ms/step - accuracy: 0.6570 - loss: 0.7773\n",
            "Epoch 5/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 56ms/step - accuracy: 0.6560 - loss: 0.7789\n",
            "Epoch 6/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 44ms/step - accuracy: 0.6566 - loss: 0.7768\n",
            "Epoch 7/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 57ms/step - accuracy: 0.6540 - loss: 0.7768\n",
            "Epoch 8/8\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 57ms/step - accuracy: 0.6539 - loss: 0.7754\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7919b92dc6b0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(text, model):\n",
        "    seq = tokenizer.texts_to_sequences([text.lower()])[0]\n",
        "    seq = pad_sequences([seq], maxlen=max_len-1, padding=\"pre\")\n",
        "    pred = np.argmax(model.predict(seq), axis=-1)\n",
        "\n",
        "    for word, idx in tokenizer.word_index.items():\n",
        "        if idx == pred:\n",
        "            return word\n"
      ],
      "metadata": {
        "id": "azYMPz5vLXJI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next_word(\"deep learning is\", lstm_model))\n",
        "print(predict_next_word(\"deep learning is\", gru_model))\n",
        "print(predict_next_word(\"deep learning is\", rnn_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKtVmtrWLZyk",
        "outputId": "c581ff8c-ca14-4f5b-8b1a-38cf82580d33"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "used\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "used\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "used\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_predict(text, model):\n",
        "    if not text.strip():\n",
        "        return \"Invalid input\"\n",
        "\n",
        "    try:\n",
        "        seq = tokenizer.texts_to_sequences([text.lower()])[0]\n",
        "        if len(seq) == 0:\n",
        "            return \"No valid tokens\"\n",
        "\n",
        "        seq = pad_sequences([seq], maxlen=max_len-1, padding=\"pre\")\n",
        "        pred = np.argmax(model.predict(seq), axis=-1)\n",
        "\n",
        "        for word, idx in tokenizer.word_index.items():\n",
        "            if idx == pred:\n",
        "                return word\n",
        "    except Exception as e:\n",
        "        return str(e)\n"
      ],
      "metadata": {
        "id": "Hwc17jVFLdzv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    \"deep learning is\",\n",
        "    \"machine learning helps in\",\n",
        "    \"neural networks enable\",\n",
        "    \"natural language processing is used for\",\n",
        "    \"learning\",\n",
        "    \"quantum learning is\",\n",
        "    \"\",\n",
        "    \"@@@@ #### !!!\"\n",
        "]\n",
        "\n",
        "models = {\n",
        "    \"RNN\": rnn_model,\n",
        "    \"LSTM\": lstm_model,\n",
        "    \"GRU\": gru_model\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n--- Testing {name} ---\")\n",
        "    for text in test_cases:\n",
        "        print(f\"Input: '{text}' â†’ Output: {safe_predict(text, model)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SH1m-rpLqYB",
        "outputId": "b2858593-2507-4ea9-8f07-e04a711f31a8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing RNN ---\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Input: 'deep learning is' â†’ Output: used\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Input: 'machine learning helps in' â†’ Output: text\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Input: 'neural networks enable' â†’ Output: text\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Input: 'natural language processing is used for' â†’ Output: automation\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Input: 'learning' â†’ Output: science\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "Input: 'quantum learning is' â†’ Output: used\n",
            "Input: '' â†’ Output: Invalid input\n",
            "Input: '@@@@ #### !!!' â†’ Output: No valid tokens\n",
            "\n",
            "--- Testing LSTM ---\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "Input: 'deep learning is' â†’ Output: used\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Input: 'machine learning helps in' â†’ Output: automation\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Input: 'neural networks enable' â†’ Output: prediction\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "Input: 'natural language processing is used for' â†’ Output: prediction\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Input: 'learning' â†’ Output: science\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Input: 'quantum learning is' â†’ Output: used\n",
            "Input: '' â†’ Output: Invalid input\n",
            "Input: '@@@@ #### !!!' â†’ Output: No valid tokens\n",
            "\n",
            "--- Testing GRU ---\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "Input: 'deep learning is' â†’ Output: used\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "Input: 'machine learning helps in' â†’ Output: automation\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "Input: 'neural networks enable' â†’ Output: prediction\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Input: 'natural language processing is used for' â†’ Output: prediction\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "Input: 'learning' â†’ Output: science\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "Input: 'quantum learning is' â†’ Output: used\n",
            "Input: '' â†’ Output: Invalid input\n",
            "Input: '@@@@ #### !!!' â†’ Output: No valid tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import json\n",
        "\n",
        "lstm_model.save(\"model/lstm_model.h5\")\n",
        "pickle.dump(tokenizer, open(\"model/tokenizer.pkl\", \"wb\"))\n",
        "\n",
        "with open(\"model/config.json\", \"w\") as f:\n",
        "    json.dump({\"max_len\": max_len}, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOEFU7qcLtw7",
        "outputId": "0d130de4-ac72-44c6-978d-feb34e713d69"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_WxL5bGIU9nC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}